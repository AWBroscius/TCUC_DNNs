# -*- coding: utf-8 -*-
"""Copy of PL_TimeSeries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e8K_SvzirjdUq5ZhO3b7VqnXAs_tCViM
"""
import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertLMHeadModel, Trainer, TrainingArguments
import numpy as np
import os

datapath = sys.argv[1]

Time_series_X_train = np.load(os.path.join(datapath, r'T_s_X_train.npy'))
data = pd.read_csv(os.path.join(datapath, r'Y_norm_transformer.csv'))
print("Data loaded! Length: ", len(data))

# Custom Dataset class
class TimeSeriesDataset(Dataset):
    def __init__(self, data, tokenizer, input_seq_length, target_seq_length):
        self.data = data
        self.tokenizer = tokenizer
        self.input_seq_length = input_seq_length
        self.target_seq_length = target_seq_length

    def __len__(self):
        return len(self.data) - self.input_seq_length - self.target_seq_length + 1

    def __getitem__(self, idx):
        x = self.data.iloc[idx:idx+self.input_seq_length].values.flatten()
        y = self.data.iloc[idx+self.input_seq_length:idx+self.target_seq_length].values.flatten()
        x_tokenized = self.tokenizer(' '.join(map(str, x)), truncation=True, padding='max_length', max_length=512, return_tensors='pt')
        y_tokenized = self.tokenizer(' '.join(map(str, y)), truncation=True, padding='max_length', max_length=512, return_tensors='pt')

        # Ensure the labels are correctly formatted
        labels = y_tokenized['input_ids'].squeeze()

        return {
            'input_ids': x_tokenized['input_ids'].squeeze(),
            'attention_mask': x_tokenized['attention_mask'].squeeze(),
            'labels': labels
        }

# Load and preprocess your data
#data = pd.read_csv('')

# Convert all columns to string
for col in data.columns:
    data[col] = data[col].astype(str)

# Tokenize the data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define input and target sequence lengths
input_seq_length = 8 * 24  # Example: 8 days of hourly data
target_seq_length = 24    # Example: Predict the next 24 hours

dataset = TimeSeriesDataset(data, tokenizer, input_seq_length, target_seq_length)

# Define DataLoader
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)

# Define the model
model = BertLMHeadModel.from_pretrained('bert-base-uncased', num_labels=1)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=int(sys.argv[2]),
    per_device_train_batch_size=100,
    per_device_eval_batch_size=100,
    warmup_steps=500,
    weight_decay=0.01,
    # logging_dir='./logs',
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train and evaluate
trainer.train()
trainer.evaluate()

# Save the model
trainer.save_model('./fine_tuned_bert')

# WandB passward = 27a62163832437994d33611c0e2142cd97fc0003

print("Training Completed!")
# """# Inference"""
#
# import torch
# from transformers import BertTokenizer, BertForSequenceClassification
#
# # Load the fine-tuned model and tokenizer
# model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
#
# # Prepare your input data (example: a new 8-day sequence for inference)
# new_data = [
#     # Add your new 8-day sequence here
#     '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8',
#     '0.9', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6',
#     # ... Continue for 8 days
# ]
# input_sequence = ' '.join(new_data)
#
# # Tokenize the input sequence
# inputs = tokenizer(input_sequence, return_tensors='pt', truncation=True, padding='max_length', max_length=512)
#
# # Set the model to evaluation mode
# model.eval()
#
# # Perform inference
# with torch.no_grad():
#     outputs = model(**inputs)
#     predictions = outputs.logits
#
# # Process the predictions (for example, converting to a readable format)
# predicted_sequence = tokenizer.decode(predictions[0], skip_special_tokens=True)
#
# print("Predicted Sequence:", predicted_sequence)